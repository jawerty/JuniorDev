{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "UAmSLULWmBf5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "ulmk159smAEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U7sMN3dGTbD",
        "outputId": "c5101311-f62a-4ad9-853b-b61ded83c364"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/86.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.32.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=6ff4e6458f0a058690044e3d15f906a1694977758e1ccad55494df682cb282d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "028RAgbCmFXT",
        "outputId": "1abde6b2-3d33-49ef-979a-795eafa14e54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/251.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od_LHwy7mEAW",
        "outputId": "b0e46919-cbba-46c5-899a-f1f3210feea7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://test.pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw7xdBBRmDfO",
        "outputId": "50c33ab8-7945-43f4-ce23-fa194c178c5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Collecting bitsandbytes\n",
            "  Downloading https://test-files.pythonhosted.org/packages/5c/e0/597d593ec3b6cf5ea7eb4894a545045bd95611de8a316a2a1eaa838a2459/bitsandbytes-0.39.0-py3-none-any.whl (95.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.39.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n8TbE0mAljco"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Prompt:\n",
        "  def technical_feedback_prompt(self, product_problem):\n",
        "    return \"\"\"\n",
        "      You are a full stack web developer and your role is to give technical feedback to your product manager who will give you a problem about the product\n",
        "\n",
        "      You have to analyze the problem and give your thoughts on how to implement or address this issue from the engineering perspective.\n",
        "\n",
        "      You may give an explanation of technologies that are needed, how many resources are needed and what in a high-level needs to be done to address the product concerns.\n",
        "\n",
        "      Here is the product manager's problem below:\n",
        "      %s\n",
        "\n",
        "      You must respond with your full technical feedback for the problem above and address the product manager's issue.\n",
        "    \"\"\" % (product_problem)\n",
        "\n",
        "\n",
        "  def ticket_sizing_prompt(self, ticket_description, sizes):\n",
        "    return \"\"\"\n",
        "      You are a full stack web developer and your role is to size tickets that given to you.\n",
        "      \"Ticket descriptions\" define a problem/task for you, the developer, to address\n",
        "      \"Ticket sizing\" is when you assign a ticket with a rating based on how complex/difficult the task is.\n",
        "      Your reasonings for sizing this ticket must be related to the complexity of the issue and your ability as a \"junior\" developer to solve it\n",
        "\n",
        "      Using your knowledge of full stack web developing size the ticket using one of the following numbers: %s\n",
        "\n",
        "      Each size value increases the complexity by a factor\n",
        "\n",
        "      Here's the ticket description below:\n",
        "      %s\n",
        "\n",
        "      You must only respond with the ticket size you chose (only using the sizes %s) and an explaination of why you chose this ticket size\n",
        "    \"\"\" % (sizes, ticket_description, sizes)\n",
        "\n",
        "  def bug_fix_prompt(self, codes, manager_problem):\n",
        "    return \"\"\"\n",
        "      You are a full stack web developer and your role is to fix bugs given the affected files. Some of the files might not be affected. Use your judgement to determine which files need to be fixed. And rewrite the full fixed file(s).\n",
        "\n",
        "      Below is the bug your manager is telling you need to fix:\n",
        "      %s\n",
        "\n",
        "\n",
        "      The affected code files you must fix are below.\n",
        "      %s\n",
        "\n",
        "      Only respond with the NEW full fixed code files in markdown format. Do not respond with a reiteration of the files you were given just the new fixed files including the older code but with your new additions. Also do not respond with any of the explanations about how you fixed the files.\n",
        "    \"\"\" % (manager_problem, codes)\n",
        "\n",
        "  def test_prompt(self, code, test_framework):\n",
        "    return \"\"\"\n",
        "      You are a full stack web developer and your role is to write tests for files that are given to you.\n",
        "\n",
        "      You must use the %s framework to implement the test.\n",
        "\n",
        "      Only respond with the written test in markdown format. Do not respond with any of the explanations about how you implemented it\n",
        "\n",
        "      The code you must write a test for is below:\n",
        "\n",
        "      CODE: %s\n",
        "    \"\"\" % (test_framework, code)\n",
        "\n",
        "  def bootstrap_project_prompt(self, technologies):\n",
        "    return \"\"\"\n",
        "You are a full stack web developer and your role is to setup a coding project from scratch. You must use include ALL the technologies below:\n",
        "\n",
        "%s\n",
        "\n",
        "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
        "First lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\n",
        "\n",
        "Then you will output the content of each file including ALL code.\n",
        "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
        "FILENAME is the lowercase file name including the file extension,\n",
        "LANG is the markup code block language for the code's language, and CODE is the code:\n",
        "\n",
        "**FILENAME**\n",
        "```LANG\n",
        "CODE\n",
        "```\n",
        "\n",
        "Do not comment on what every file does. Please note that the code should be fully functional. No placeholders.\n",
        "\n",
        "You will start with the \"entrypoint\" file, then go to the ones that are imported by that file, and so on.\n",
        "Please note that the code should be fully functional. No placeholders.\n",
        "\n",
        "Follow a language and framework appropriate best practice file naming convention.\n",
        "Make sure that files contain all imports, types etc.  The code should be fully functional. Make sure that code in different files are compatible with each other.\n",
        "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
        "Include module dependency or package manager dependency definition file.\n",
        "Before you finish, double check that all parts of the architecture is present in the files.\n",
        "\n",
        "    \"\"\" % technologies"
      ],
      "metadata": {
        "id": "k0ODfptmu2KV"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"WizardLM/WizardCoder-15B-V1.0\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": Accelerator().process_index},\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "pecNOu2bxQR0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from accelerate import Accelerator\n",
        "\n",
        "class CoderLLM:\n",
        "  def __init__(self, prompt, model_name=\"bigcode/starcoder\"):\n",
        "    self.model_name = model_name\n",
        "    self.load_8bit = True\n",
        "    self.model = None;\n",
        "    self.tokenizer = None;\n",
        "    self.prompt = prompt\n",
        "    self.base_prompt = \"\"\"\n",
        "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    %s\n",
        "\n",
        "    ### Response:\n",
        "    \"\"\"\n",
        "\n",
        "  def load(self):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def get_instruction_prompt(self, instruction):\n",
        "    return self.base_prompt % instruction\n",
        "\n",
        "  def get_write_test_prompt(self, code, test_framework):\n",
        "    return self.get_instruction_prompt(self.prompt.test_prompt(code, test_framework))\n",
        "\n",
        "  def get_bootstrap_project_prompt(self, technologies):\n",
        "    return self.get_instruction_prompt(self.prompt.bootstrap_project_prompt(technologies))\n",
        "\n",
        "  def get_fix_bug_prompt(self, codes, manager_problem):\n",
        "    return self.get_instruction_prompt(self.prompt.bug_fix_prompt(codes, manager_problem))\n",
        "\n",
        "  def get_ticket_sizing_prompt(self, ticket_description, sizes):\n",
        "    return self.get_instruction_prompt(self.prompt.ticket_sizing_prompt(ticket_description, sizes))\n",
        "\n",
        "  def get_technical_feedback_prompt(self, product_problem):\n",
        "    return self.get_instruction_prompt(self.prompt.technical_feedback_prompt(product_problem))\n",
        "\n",
        "  def generate(self, instruction,\n",
        "        input=None,\n",
        "        temperature=1,\n",
        "        top_p=0.9,\n",
        "        top_k=40,\n",
        "        num_beams=1,\n",
        "        max_new_tokens=2048,\n",
        "        **kwargs,):\n",
        "    inputs = self.tokenizer(instruction, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        do_sample=True,\n",
        "        eos_token_id=self.tokenizer.eos_token_id,\n",
        "        pad_token_id=self.tokenizer.pad_token_id,\n",
        "        **kwargs,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        generation_output = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences\n",
        "    output = self.tokenizer.batch_decode(s, skip_special_tokens=True)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "-YCJKdtjlkS-"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = Prompt()"
      ],
      "metadata": {
        "id": "u-w4KJGBxF63"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading the LLM:\", model_name)\n",
        "coder_llm = CoderLLM(prompt, model_name=model_name)\n",
        "coder_llm.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTtJpzXIlyKp",
        "outputId": "949d0ec2-d832-435a-dda7-1931fc178b66"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the LLM: WizardLM/WizardCoder-15B-V1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jawerty/threadwatch.xyz.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BAwDlqStTpq",
        "outputId": "798f5410-74c8-40fb-e6a8-16c4e6bef9ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'threadwatch.xyz'...\n",
            "remote: Enumerating objects: 343, done.\u001b[K\n",
            "remote: Total 343 (delta 0), reused 0 (delta 0), pack-reused 343\u001b[K\n",
            "Receiving objects: 100% (343/343), 1.93 MiB | 27.79 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls threadwatch.xyz/server/frontend/components"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9HoyaL9t1rC",
        "outputId": "6cfe67a7-771a-4008-e012-e0611808efc7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "App.js\t\t  Header.js\t\tOnboardingFlow.js  Thread.js\n",
            "CommenterView.js  LinkGeneratorForm.js\tReactionVoter.js   ThreadList.js\n",
            "FrontPage.js\t  Loading.js\t\tThreadCard.js\t   TopicSelector.js\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "import re\n",
        "import glob\n",
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "class JuniorDev:\n",
        "  def __init__(self, coder_llm):\n",
        "    self.coder_llm = coder_llm\n",
        "    self.sentence_transformers = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
        "\n",
        "  def write_test(self, filename, test_framework=\"jest\"):\n",
        "    print(\"Writing test for\", filename)\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "      code = \"\".join(lines)\n",
        "      code = code[:4096]\n",
        "\n",
        "    prompt = self.coder_llm.get_write_test_prompt(code, test_framework)\n",
        "\n",
        "    output = self.coder_llm.generate(prompt)[0]\n",
        "\n",
        "    output = output[output.index(\"### Response:\"):]\n",
        "    test_code = re.findall(r\"```(.*?)```\", output, re.DOTALL)[0]\n",
        "    return test_code\n",
        "    # generate here\n",
        "\n",
        "  def bootstrap_project(self, technologies):\n",
        "    technologies = \"\\n\".join([f\"- {technology}\" for technology in technologies])\n",
        "\n",
        "    prompt = self.coder_llm.get_bootstrap_project_prompt(technologies)\n",
        "\n",
        "    output = self.coder_llm.generate(prompt, max_new_tokens=8000)[0]\n",
        "\n",
        "    output = output[output.index(\"### Response:\"):]\n",
        "    print(output)\n",
        "    code_blocks = re.findall(r\"```(.*?)```\", output, re.DOTALL)\n",
        "    file_names = re.findall(r\"\\*\\*(.*?)\\*\\*\", output, re.DOTALL)\n",
        "    code_files = []\n",
        "    for i in range(0, len(file_names)):\n",
        "      if i < len(code_blocks):\n",
        "        code_files.append({\n",
        "            \"file_name\": file_names[i],\n",
        "            \"code_block\": code_blocks[i]\n",
        "        })\n",
        "\n",
        "    for file_code_pair in code_files:\n",
        "      filepath = \"bootstrapped-project/\"+file_code_pair[\"file_name\"]\n",
        "      os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "      with open(filepath, \"w+\") as f:\n",
        "        code_block = file_code_pair[\"code_block\"].split(\"\\n\")[1:]\n",
        "        f.write(\"\\n\".join(code_block).encode('ascii', 'ignore').decode('ascii'))\n",
        "\n",
        "\n",
        "    # code_blocks = re.findall(r\"```(.*?)```\", output, re.DOTALL)\n",
        "\n",
        "    # print(code_blocks)\n",
        "\n",
        "  def fix_bug(self, folder, manager_problem):\n",
        "    code_embeddings = []\n",
        "\n",
        "    ignore_folders = [\"node_modules/\", \"env/\"]\n",
        "    ignore_files = [\".json\", \".txt\", \".md\"]\n",
        "\n",
        "    for filename in glob.iglob(f\"./{folder}/**/*\", recursive=True):\n",
        "      ignore_folder = len(list(filter(lambda x: x in filename, ignore_folders))) > 0\n",
        "      ignore_file = len(list(filter(lambda x: x in filename, ignore_files))) > 0\n",
        "      is_a_file = \".\" in filename.split(\"/\")[-1]\n",
        "      if ignore_folder or ignore_file or not is_a_file:\n",
        "        print(\"ignore\", filename)\n",
        "        continue\n",
        "\n",
        "      print(\"Embeddings for\", filename)\n",
        "      with open(filename, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        code = \"\".join(lines)\n",
        "        code = code[:4096]\n",
        "\n",
        "      embedding = self.sentence_transformers.encode(f\"{filename}\\n{code}\")\n",
        "      code_embeddings.append({\n",
        "          \"embedding\": embedding,\n",
        "          \"filename\": filename,\n",
        "          \"code\": f\"New File: {filename}\\n{code}\"\n",
        "      })\n",
        "\n",
        "    query = self.sentence_transformers.encode(manager_problem)\n",
        "\n",
        "    new_embeddings = []\n",
        "    for embed in code_embeddings:\n",
        "      embed[\"score\"] = util.dot_score(query, embed[\"embedding\"]).item()\n",
        "      new_embeddings.append(embed)\n",
        "\n",
        "    del code_embeddings\n",
        "\n",
        "    top_files = sorted(new_embeddings, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "    print(\"Affected files\")\n",
        "    for _file in top_files:\n",
        "      print(_file[\"filename\"])\n",
        "\n",
        "    top_matching_files = \"\\n\".join(list(map(lambda x: x[\"code\"], top_files)))\n",
        "\n",
        "    prompt = self.coder_llm.get_fix_bug_prompt(top_matching_files, manager_problem)\n",
        "    output = self.coder_llm.generate(prompt, max_new_tokens=4096)[0]\n",
        "    output = output[output.index(\"### Response:\"):]\n",
        "    print(output)\n",
        "\n",
        "  def size_ticket(self, ticket_desription, sizes=[1,2,3,5,8]):\n",
        "    prompt = self.coder_llm.get_ticket_sizing_prompt(ticket_desription, \", \".join([str(x) for x in sizes]))\n",
        "    output = self.coder_llm.generate(prompt, max_new_tokens=4096)[0]\n",
        "    output = output[output.index(\"### Response:\"):]\n",
        "    print(output)\n",
        "    # vecorize code\n",
        "    # for every file in the codebase vectorize all the files and fine the file that matches the problem the most\n",
        "\n",
        "  def technical_feedback(self, product_problem):\n",
        "    prompt = self.coder_llm.get_technical_feedback_prompt(product_problem)\n",
        "    output = self.coder_llm.generate(prompt, max_new_tokens=2048)[0]\n",
        "    output = output[output.index(\"### Response:\"):]\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "CLrfPgd2s7Uj"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "junior_dev = JuniorDev(coder_llm)\n",
        "# test_code = junior_dev.write_test(\"./threadwatch.xyz/server/frontend/components/LinkGeneratorForm.js\", test_framework=\"enzyme\")\n",
        "\n",
        "# print(\"\\n\\nFinal test:\\n\\n\")\n",
        "# print(test_code)\n",
        "\n",
        "# output = junior_dev.bootstrap_project([\"react\", \"jest\", \"webpack\", \"tailwind\"])\n",
        "\n",
        "# output = junior_dev.fix_bug(\"./threadwatch.xyz\", \"The header file is styled weirdly. I want all the css to be using flex for positioning\")\n",
        "\n",
        "junior_dev.technical_feedback(\"As a product manager for an ecommerce website. I want to have an interal tool that is a browser extension that allows me to see the full database information for products that I see on the product page. How can we do this?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcIH80ynw7pI",
        "outputId": "964325ae-59ca-4e21-ba49-ff943a42e62e"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Response:\n",
            "    \r\n",
            "      1. The issue is that the internal tool is a browser extension, but the product manager wants to have it for mobile devices as well. So, it is not feasible to create a browser extension for mobile devices. Instead, they need to create a mobile app or integrate a mobile SDK with their existing website.\r\n",
            "       \r\n",
            "      2. To address this, the product manager can follow these steps:\r\n",
            "          a. Research mobile ecommerce solutions, such as Shopify, WooCommerce, and Magento Mobile.\r\n",
            "          b. Choose a mobile ecommerce solution and integrate it with their website.\r\n",
            "          c. Implement the internal tool as a mobile feature that allows users to see full product details.\r\n",
            "          \r\n",
            "      3. To implement the internal tool, the product manager can use a frontend framework such as React Native, Ionic, or Flutter. They also need to research and choose a database for storing product information.\r\n",
            "       \r\n",
            "      4. Finally, the product manager can create a user interface for the tool that allows users to see product details, such as product name, price, description, and reviews. They can also add product to a cart or wishlist.\n"
          ]
        }
      ]
    }
  ]
}